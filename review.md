## Abstract

초기에는 순환구조(recurrent)나 Encoder-Decoder 구조를 가진 RNN이나 LSTM 등의 모델들이 번역 모델에 있어서 큰 성과를 내고 있었으나,
이들은 대량의 Memory와 많은 Computation을 요구하고 있고, 문장의 길이가 길어질수록 한계를 드러내고 있다.

이 문제점을 보완하며 기존 Encoder-Decoder구조를 벗어난 Attention Mechanism만으로 구성된 Transformer 모델이 발표되었다.
이는 성능과 메모리 두 가지 측면에서 우세하였으며 현재 Transformer는 컴퓨터 비전분야에도 쓰이고 있다.

---

## Structure

